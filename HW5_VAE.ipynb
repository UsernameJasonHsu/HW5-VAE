{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-vN6RVkQoWJD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import datasets, Sequential, layers, metrics, optimizers, losses\n",
        "\n",
        "tf.random.set_seed(22)\n",
        "np.random.seed(22)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image(imgs, name):\n",
        "  new_im = Image.new('L', (280, 280))\n",
        "  index = 0\n",
        "  for i in range(0, 280, 28):\n",
        "    for j in range(0, 280, 28):\n",
        "      im = imgs[index]\n",
        "      im = Image.fromarray(im, mode='L')\n",
        "      new_im.paste(im, (i,j))\n",
        "      index += 1\n",
        "  new_im.save(name)"
      ],
      "metadata": {
        "id": "pDNCPEovpgky"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_dim = 20\n",
        "z_dim = 10\n",
        "batches = 512"
      ],
      "metadata": {
        "id": "a-Ekt0CrqLfG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "x_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n",
        "\n",
        "train_db = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_db = train_db.shuffle(batches*5).batch(batches)\n",
        "\n",
        "test_db = tf.data.Dataset.from_tensor_slices(x_test)\n",
        "test_db = test_db.batch(batches)\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMOAP1UZrE9_",
        "outputId": "19229ec5-95b2-4d5c-bc5d-a308f772b167"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n",
            "(60000, 28, 28) (60000,)\n",
            "(10000, 28, 28) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(VAE, self).__init__()\n",
        "    # Encoders\n",
        "    self.fc1 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.fc2 = layers.Dense(z_dim)\n",
        "    self.fc3 = layers.Dense(z_dim)\n",
        "\n",
        "    #Decoders\n",
        "    self.fc4 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.fc5 = layers.Dense(784)\n",
        "\n",
        "  def encoder(self, x):\n",
        "    h = self.fc1(x)\n",
        "    mu = self.fc2(h)\n",
        "    log_var = self.fc3(h)\n",
        "    return mu, log_var\n",
        "\n",
        "  def decoder(self, z):\n",
        "    out = self.fc4(z)\n",
        "    out = self.fc5(out)\n",
        "    return out\n",
        "\n",
        "  def reparameterize(self, mu, log_var):\n",
        "    esp = tf.random.normal(log_var.shape)\n",
        "    std = tf.exp(log_var*0.5)\n",
        "    z = mu + std * esp\n",
        "    return z\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    # [b, 784] -> [b, z_dim], [b, z_dim]\n",
        "    mu, log_var = self.encoder(inputs)\n",
        "\n",
        "    z = self.reparameterize(mu, log_var)\n",
        "    x_hat = self.decoder(z)\n",
        "    return x_hat, mu, log_var"
      ],
      "metadata": {
        "id": "iq3_mb_nr3km"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VAE()\n",
        "model.build(input_shape = (4,784))\n",
        "model.summary()\n",
        "\n",
        "optimizer = optimizers.Adam(lr=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeAHeuZiuTQv",
        "outputId": "4f2a9fa4-6c4d-4dde-9ef1-5359048d13b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vae\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               multiple                  100480    \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  1290      \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  1290      \n",
            "                                                                 \n",
            " dense_3 (Dense)             multiple                  1408      \n",
            "                                                                 \n",
            " dense_4 (Dense)             multiple                  101136    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 205604 (803.14 KB)\n",
            "Trainable params: 205604 (803.14 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):\n",
        "  for step, x in enumerate(train_db):\n",
        "    x = tf.reshape(x, [-1, 784])\n",
        "    with tf.GradientTape() as tape:\n",
        "      x_rec_logits, mu, log_var = model(x)\n",
        "      rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_rec_logits)\n",
        "      rec_loss = tf.reduce_sum(rec_loss) / x.shape[0]\n",
        "\n",
        "      kl_div = -0.5*(log_var+1-mu**2-tf.exp(log_var))\n",
        "      kl_div = tf.reduce_sum(kl_div) / x.shape[0]\n",
        "\n",
        "      loss = rec_loss + 1.*kl_div\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    if step % 100 == 0:\n",
        "      print('epoch:', epoch, 'step:', step, 'kl_div:', float(kl_div), 'rec loss:', float(rec_loss))\n",
        "\n",
        "    z = tf.random.normal((batches, z_dim))\n",
        "    logits = model.decoder(z)\n",
        "    x_hat = tf.sigmoid(logits)\n",
        "    x_hat = tf.reshape(x_hat, [-1,28,28]).numpy() * 255.\n",
        "    x_hat = x_hat.astype(np.uint8)\n",
        "    save_image(x_hat, '/content/drive/MyDrive/NCHU/碩二/深度生成模型/vae_images/sampled_epoch%d.png' % epoch)\n",
        "\n",
        "    x = next(iter(test_db))\n",
        "    x = tf.reshape(x, [-1,784])\n",
        "    x_hat_logits, _, _ = model(x)\n",
        "    x_hat = tf.sigmoid(x_hat_logits)\n",
        "    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255.\n",
        "    x_hat = x_hat.astype(np.uint8)\n",
        "    save_image(x_hat, '/content/drive/MyDrive/NCHU/碩二/深度生成模型/vae_images/rec_epoch%d.png' % epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmd7SSBqugcb",
        "outputId": "d315d483-d98e-4b4a-e547-f7029392fa8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x79f0f1d9c670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x79f0f1d9c670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 step: 0 kl_div: 1.9156365394592285 rec loss: 546.4342651367188\n",
            "epoch: 0 step: 100 kl_div: 15.764577865600586 rec loss: 285.8543395996094\n",
            "epoch: 1 step: 0 kl_div: 15.772905349731445 rec loss: 271.75079345703125\n",
            "epoch: 1 step: 100 kl_div: 15.878440856933594 rec loss: 258.23291015625\n",
            "epoch: 2 step: 0 kl_div: 15.695530891418457 rec loss: 249.97988891601562\n",
            "epoch: 2 step: 100 kl_div: 14.598577499389648 rec loss: 254.60684204101562\n",
            "epoch: 3 step: 0 kl_div: 14.87392807006836 rec loss: 250.02096557617188\n",
            "epoch: 3 step: 100 kl_div: 15.078659057617188 rec loss: 249.062255859375\n",
            "epoch: 4 step: 0 kl_div: 14.45456314086914 rec loss: 245.77304077148438\n",
            "epoch: 4 step: 100 kl_div: 15.277034759521484 rec loss: 246.81369018554688\n",
            "epoch: 5 step: 0 kl_div: 14.87739086151123 rec loss: 237.3536834716797\n",
            "epoch: 5 step: 100 kl_div: 15.063006401062012 rec loss: 240.16671752929688\n",
            "epoch: 6 step: 0 kl_div: 15.251806259155273 rec loss: 236.96929931640625\n",
            "epoch: 6 step: 100 kl_div: 14.844560623168945 rec loss: 242.22352600097656\n",
            "epoch: 7 step: 0 kl_div: 14.992895126342773 rec loss: 235.5195770263672\n",
            "epoch: 7 step: 100 kl_div: 14.944143295288086 rec loss: 234.10986328125\n",
            "epoch: 8 step: 0 kl_div: 14.459220886230469 rec loss: 240.87527465820312\n",
            "epoch: 8 step: 100 kl_div: 15.085334777832031 rec loss: 241.6995086669922\n",
            "epoch: 9 step: 0 kl_div: 14.55569076538086 rec loss: 233.04129028320312\n",
            "epoch: 9 step: 100 kl_div: 15.147863388061523 rec loss: 232.61721801757812\n",
            "epoch: 10 step: 0 kl_div: 15.075855255126953 rec loss: 240.16537475585938\n",
            "epoch: 10 step: 100 kl_div: 15.345162391662598 rec loss: 234.1834716796875\n",
            "epoch: 11 step: 0 kl_div: 15.15841293334961 rec loss: 230.01776123046875\n",
            "epoch: 11 step: 100 kl_div: 14.858757019042969 rec loss: 235.23696899414062\n",
            "epoch: 12 step: 0 kl_div: 15.062884330749512 rec loss: 230.85678100585938\n",
            "epoch: 12 step: 100 kl_div: 14.530854225158691 rec loss: 237.2626953125\n",
            "epoch: 13 step: 0 kl_div: 14.859498977661133 rec loss: 229.65481567382812\n",
            "epoch: 13 step: 100 kl_div: 15.315598487854004 rec loss: 233.0809326171875\n",
            "epoch: 14 step: 0 kl_div: 14.989033699035645 rec loss: 230.08059692382812\n",
            "epoch: 14 step: 100 kl_div: 15.17993450164795 rec loss: 238.34889221191406\n",
            "epoch: 15 step: 0 kl_div: 14.47020149230957 rec loss: 234.79371643066406\n",
            "epoch: 15 step: 100 kl_div: 14.435066223144531 rec loss: 231.89231872558594\n",
            "epoch: 16 step: 0 kl_div: 14.672261238098145 rec loss: 225.96009826660156\n",
            "epoch: 16 step: 100 kl_div: 14.821195602416992 rec loss: 232.58724975585938\n",
            "epoch: 17 step: 0 kl_div: 14.603005409240723 rec loss: 229.2073974609375\n",
            "epoch: 17 step: 100 kl_div: 14.682893753051758 rec loss: 231.5736846923828\n",
            "epoch: 18 step: 0 kl_div: 14.94072151184082 rec loss: 229.2334442138672\n",
            "epoch: 18 step: 100 kl_div: 14.831783294677734 rec loss: 236.8227081298828\n",
            "epoch: 19 step: 0 kl_div: 15.052102088928223 rec loss: 229.670166015625\n",
            "epoch: 19 step: 100 kl_div: 14.939440727233887 rec loss: 231.24293518066406\n",
            "epoch: 20 step: 0 kl_div: 14.763260841369629 rec loss: 234.08790588378906\n",
            "epoch: 20 step: 100 kl_div: 15.044693946838379 rec loss: 234.25778198242188\n",
            "epoch: 21 step: 0 kl_div: 15.348050117492676 rec loss: 227.24049377441406\n",
            "epoch: 21 step: 100 kl_div: 14.684219360351562 rec loss: 238.32540893554688\n",
            "epoch: 22 step: 0 kl_div: 14.917862892150879 rec loss: 228.74453735351562\n",
            "epoch: 22 step: 100 kl_div: 14.92075252532959 rec loss: 229.78271484375\n",
            "epoch: 23 step: 0 kl_div: 15.193113327026367 rec loss: 227.880859375\n",
            "epoch: 23 step: 100 kl_div: 15.359428405761719 rec loss: 233.75022888183594\n",
            "epoch: 24 step: 0 kl_div: 14.840985298156738 rec loss: 233.27537536621094\n",
            "epoch: 24 step: 100 kl_div: 15.00228500366211 rec loss: 230.96856689453125\n",
            "epoch: 25 step: 0 kl_div: 15.3026123046875 rec loss: 226.5456085205078\n",
            "epoch: 25 step: 100 kl_div: 14.982908248901367 rec loss: 232.61048889160156\n",
            "epoch: 26 step: 0 kl_div: 15.202425003051758 rec loss: 230.15684509277344\n",
            "epoch: 26 step: 100 kl_div: 14.952372550964355 rec loss: 231.51678466796875\n",
            "epoch: 27 step: 0 kl_div: 14.435710906982422 rec loss: 230.06314086914062\n",
            "epoch: 27 step: 100 kl_div: 14.795438766479492 rec loss: 229.5673370361328\n",
            "epoch: 28 step: 0 kl_div: 15.422558784484863 rec loss: 228.837646484375\n",
            "epoch: 28 step: 100 kl_div: 14.916277885437012 rec loss: 226.96002197265625\n",
            "epoch: 29 step: 0 kl_div: 14.696746826171875 rec loss: 225.1304931640625\n",
            "epoch: 29 step: 100 kl_div: 15.083704948425293 rec loss: 224.831787109375\n",
            "epoch: 30 step: 0 kl_div: 14.618528366088867 rec loss: 227.00010681152344\n",
            "epoch: 30 step: 100 kl_div: 14.931853294372559 rec loss: 229.5040283203125\n",
            "epoch: 31 step: 0 kl_div: 15.014123916625977 rec loss: 225.68222045898438\n",
            "epoch: 31 step: 100 kl_div: 15.069265365600586 rec loss: 230.00369262695312\n",
            "epoch: 32 step: 0 kl_div: 14.90235710144043 rec loss: 221.5416259765625\n",
            "epoch: 32 step: 100 kl_div: 15.047947883605957 rec loss: 234.20339965820312\n",
            "epoch: 33 step: 0 kl_div: 14.374052047729492 rec loss: 225.95252990722656\n",
            "epoch: 33 step: 100 kl_div: 14.6019926071167 rec loss: 233.9949188232422\n",
            "epoch: 34 step: 0 kl_div: 15.16979694366455 rec loss: 229.21473693847656\n",
            "epoch: 34 step: 100 kl_div: 14.960809707641602 rec loss: 232.0028839111328\n",
            "epoch: 35 step: 0 kl_div: 15.029655456542969 rec loss: 227.5537109375\n",
            "epoch: 35 step: 100 kl_div: 14.670758247375488 rec loss: 226.12945556640625\n",
            "epoch: 36 step: 0 kl_div: 15.156745910644531 rec loss: 225.89093017578125\n",
            "epoch: 36 step: 100 kl_div: 15.243402481079102 rec loss: 233.64820861816406\n",
            "epoch: 37 step: 0 kl_div: 14.964231491088867 rec loss: 227.79737854003906\n",
            "epoch: 37 step: 100 kl_div: 15.050277709960938 rec loss: 226.51504516601562\n",
            "epoch: 38 step: 0 kl_div: 14.769567489624023 rec loss: 228.95455932617188\n",
            "epoch: 38 step: 100 kl_div: 14.699519157409668 rec loss: 224.6544952392578\n",
            "epoch: 39 step: 0 kl_div: 14.58102035522461 rec loss: 224.4754180908203\n",
            "epoch: 39 step: 100 kl_div: 14.921213150024414 rec loss: 225.44235229492188\n",
            "epoch: 40 step: 0 kl_div: 15.371522903442383 rec loss: 227.9323272705078\n",
            "epoch: 40 step: 100 kl_div: 15.430140495300293 rec loss: 230.71115112304688\n",
            "epoch: 41 step: 0 kl_div: 15.715921401977539 rec loss: 232.0032501220703\n",
            "epoch: 41 step: 100 kl_div: 15.260449409484863 rec loss: 224.5476837158203\n",
            "epoch: 42 step: 0 kl_div: 14.600154876708984 rec loss: 228.2321319580078\n",
            "epoch: 42 step: 100 kl_div: 15.000636100769043 rec loss: 231.09585571289062\n",
            "epoch: 43 step: 0 kl_div: 15.041115760803223 rec loss: 224.48548889160156\n",
            "epoch: 43 step: 100 kl_div: 15.004646301269531 rec loss: 233.1525421142578\n",
            "epoch: 44 step: 0 kl_div: 15.038268089294434 rec loss: 227.5000762939453\n",
            "epoch: 44 step: 100 kl_div: 14.784894943237305 rec loss: 227.2933807373047\n",
            "epoch: 45 step: 0 kl_div: 16.085737228393555 rec loss: 225.40676879882812\n",
            "epoch: 45 step: 100 kl_div: 14.965462684631348 rec loss: 230.71090698242188\n",
            "epoch: 46 step: 0 kl_div: 14.719549179077148 rec loss: 226.46319580078125\n",
            "epoch: 46 step: 100 kl_div: 14.91347599029541 rec loss: 230.6887969970703\n",
            "epoch: 47 step: 0 kl_div: 15.413045883178711 rec loss: 227.04794311523438\n",
            "epoch: 47 step: 100 kl_div: 14.891580581665039 rec loss: 230.2697296142578\n",
            "epoch: 48 step: 0 kl_div: 14.96960735321045 rec loss: 223.06069946289062\n",
            "epoch: 48 step: 100 kl_div: 14.89896011352539 rec loss: 229.6715087890625\n",
            "epoch: 49 step: 0 kl_div: 14.702619552612305 rec loss: 224.3346710205078\n",
            "epoch: 49 step: 100 kl_div: 14.96095085144043 rec loss: 226.67872619628906\n",
            "epoch: 50 step: 0 kl_div: 14.545329093933105 rec loss: 226.3048095703125\n",
            "epoch: 50 step: 100 kl_div: 14.553569793701172 rec loss: 230.4329833984375\n",
            "epoch: 51 step: 0 kl_div: 14.96375846862793 rec loss: 229.3494415283203\n",
            "epoch: 51 step: 100 kl_div: 15.372156143188477 rec loss: 227.534912109375\n",
            "epoch: 52 step: 0 kl_div: 14.776671409606934 rec loss: 227.51734924316406\n",
            "epoch: 52 step: 100 kl_div: 15.179628372192383 rec loss: 225.39047241210938\n",
            "epoch: 53 step: 0 kl_div: 14.853658676147461 rec loss: 220.8172149658203\n",
            "epoch: 53 step: 100 kl_div: 14.66599178314209 rec loss: 224.36663818359375\n",
            "epoch: 54 step: 0 kl_div: 15.2471284866333 rec loss: 222.41275024414062\n",
            "epoch: 54 step: 100 kl_div: 15.767049789428711 rec loss: 223.98623657226562\n",
            "epoch: 55 step: 0 kl_div: 14.736133575439453 rec loss: 224.25123596191406\n",
            "epoch: 55 step: 100 kl_div: 15.166834831237793 rec loss: 224.6210174560547\n",
            "epoch: 56 step: 0 kl_div: 14.416059494018555 rec loss: 220.98654174804688\n",
            "epoch: 56 step: 100 kl_div: 14.827313423156738 rec loss: 224.47503662109375\n",
            "epoch: 57 step: 0 kl_div: 14.97838306427002 rec loss: 225.28309631347656\n",
            "epoch: 57 step: 100 kl_div: 14.910905838012695 rec loss: 229.7452850341797\n",
            "epoch: 58 step: 0 kl_div: 15.41225814819336 rec loss: 226.24327087402344\n",
            "epoch: 58 step: 100 kl_div: 14.964496612548828 rec loss: 225.61257934570312\n",
            "epoch: 59 step: 0 kl_div: 14.889009475708008 rec loss: 228.5882110595703\n",
            "epoch: 59 step: 100 kl_div: 15.124740600585938 rec loss: 228.9664306640625\n",
            "epoch: 60 step: 0 kl_div: 14.639838218688965 rec loss: 225.99720764160156\n",
            "epoch: 60 step: 100 kl_div: 14.716653823852539 rec loss: 228.3778533935547\n",
            "epoch: 61 step: 0 kl_div: 15.018570899963379 rec loss: 221.2975311279297\n",
            "epoch: 61 step: 100 kl_div: 15.144963264465332 rec loss: 227.30322265625\n",
            "epoch: 62 step: 0 kl_div: 15.411157608032227 rec loss: 226.20494079589844\n",
            "epoch: 62 step: 100 kl_div: 14.939616203308105 rec loss: 231.94375610351562\n",
            "epoch: 63 step: 0 kl_div: 15.17278003692627 rec loss: 226.23583984375\n",
            "epoch: 63 step: 100 kl_div: 14.866781234741211 rec loss: 225.75575256347656\n",
            "epoch: 64 step: 0 kl_div: 14.925232887268066 rec loss: 227.08717346191406\n",
            "epoch: 64 step: 100 kl_div: 14.633966445922852 rec loss: 221.1320037841797\n",
            "epoch: 65 step: 0 kl_div: 14.625863075256348 rec loss: 222.63124084472656\n",
            "epoch: 65 step: 100 kl_div: 15.307931900024414 rec loss: 222.24818420410156\n",
            "epoch: 66 step: 0 kl_div: 14.648849487304688 rec loss: 224.35960388183594\n",
            "epoch: 66 step: 100 kl_div: 15.073565483093262 rec loss: 230.59445190429688\n",
            "epoch: 67 step: 0 kl_div: 14.685884475708008 rec loss: 221.79486083984375\n",
            "epoch: 67 step: 100 kl_div: 14.818185806274414 rec loss: 232.49423217773438\n",
            "epoch: 68 step: 0 kl_div: 15.500475883483887 rec loss: 224.24559020996094\n",
            "epoch: 68 step: 100 kl_div: 15.288686752319336 rec loss: 224.71896362304688\n",
            "epoch: 69 step: 0 kl_div: 14.778633117675781 rec loss: 230.26254272460938\n",
            "epoch: 69 step: 100 kl_div: 15.205071449279785 rec loss: 229.9610137939453\n",
            "epoch: 70 step: 0 kl_div: 14.644475936889648 rec loss: 225.35592651367188\n",
            "epoch: 70 step: 100 kl_div: 15.048809051513672 rec loss: 222.73255920410156\n",
            "epoch: 71 step: 0 kl_div: 15.309596061706543 rec loss: 225.2339324951172\n",
            "epoch: 71 step: 100 kl_div: 15.031465530395508 rec loss: 229.97116088867188\n",
            "epoch: 72 step: 0 kl_div: 14.91513442993164 rec loss: 218.5294189453125\n",
            "epoch: 72 step: 100 kl_div: 14.600732803344727 rec loss: 231.73085021972656\n",
            "epoch: 73 step: 0 kl_div: 15.439818382263184 rec loss: 225.0180206298828\n",
            "epoch: 73 step: 100 kl_div: 14.886423110961914 rec loss: 224.22413635253906\n",
            "epoch: 74 step: 0 kl_div: 15.180767059326172 rec loss: 226.7404022216797\n",
            "epoch: 74 step: 100 kl_div: 14.632620811462402 rec loss: 232.19918823242188\n"
          ]
        }
      ]
    }
  ]
}